# Semana 1: Perceptr√≥n Simple
## Conceptos Fundamentales (Notas de Clase)

### Geometr√≠a del Perceptr√≥n
- **Hiperplano**: Subvariedad de dimensi√≥n (n-1) en un espacio de n dimensiones que separa dicho espacio en dos mitades
  - En 2D: una l√≠nea separa el plano
  - En 3D: un plano separa el espacio
  - En nD: un hiperplano separa el espacio n-dimensional

- **Convexo cerrado**: Pol√≠gono que contiene todos los puntos de una clase
  - Los datos linealmente separables forman regiones convexas
  - El perceptr√≥n encuentra el hiperplano que separa estas regiones

### Operaciones Matem√°ticas Clave
- **Producto escalar/producto interno**: w^T x = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô
- **Proyecci√≥n de a sobre b**: Componente del vector a en la direcci√≥n de b
  - Utilizado en el c√°lculo de la entrada neta del perceptr√≥n

### Arquitectura Interna del Perceptr√≥n
- **h (excitaci√≥n)**: Informaci√≥n que llega a la neurona
  - h = Œ£(w·µ¢ √ó x·µ¢) + b
  - Es el producto de cada entrada por su peso correspondiente m√°s el sesgo
  
- **O (funci√≥n de activaci√≥n)**: Transforma la excitaci√≥n en la salida final
  - Para perceptr√≥n simple: funci√≥n escal√≥n
  - Decide si la neurona "se activa" o no

- **Umbral**: Valor cr√≠tico que determina la activaci√≥n
  - En perceptr√≥n simple, umbral = 0
  - Si h ‚â• umbral ‚Üí salida = 1, sino ‚Üí salida = 0

### Proceso de Entrenamiento
- **Pesos autom√°ticos**: Los pesos se calculan autom√°ticamente durante el entrenamiento
  - Inicializaci√≥n: valores aleatorios peque√±os
  - Actualizaci√≥n: regla del perceptr√≥n basada en errores
  
- **√âpoca**: Ciclo completo donde todos los ejemplos de entrenamiento han pasado por el perceptr√≥n al menos una vez
  - Una √©poca = una pasada completa por el dataset
  - El entrenamiento puede requerir m√∫ltiples √©pocas para convergir


## Introducci√≥n


El perceptr√≥n simple es la unidad b√°sica de procesamiento de las redes neuronales artificiales, inspirado en el funcionamiento de las neuronas biol√≥gicas. Fue propuesto por Frank Rosenblatt en 1957 y representa el primer modelo de neurona artificial.

## ¬øQu√© es un Perceptr√≥n?

Un perceptr√≥n es un clasificador binario lineal que puede aprender a separar dos clases de datos mediante un hiperplano en el espacio de caracter√≠sticas. Es la forma m√°s simple de una red neuronal artificial.

### Componentes del Perceptr√≥n

1. **Entradas (inputs)**: x‚ÇÅ, x‚ÇÇ, ..., x‚Çô
2. **Pesos (weights)**: w‚ÇÅ, w‚ÇÇ, ..., w‚Çô
3. **Sesgo (bias)**: b
4. **Funci√≥n de activaci√≥n**: f(z)
5. **Salida**: y

## Formulaci√≥n Matem√°tica

### Funci√≥n de Entrada Neta
La entrada neta del perceptr√≥n se calcula como:

```
z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b
```

O en forma vectorial:
```
z = w^T x + b
```

### Funci√≥n de Activaci√≥n
Para el perceptr√≥n simple, se utiliza la funci√≥n escal√≥n (step function):

```
f(z) = {
  1  si z ‚â• 0
  0  si z < 0
}
```

### Salida del Perceptr√≥n
```
y = f(w^T x + b)
```

## Algoritmo de Aprendizaje

El perceptr√≥n aprende mediante un proceso iterativo que ajusta los pesos bas√°ndose en los errores de clasificaci√≥n.

### Regla de Actualizaci√≥n de Pesos

Para cada ejemplo de entrenamiento (x·µ¢, t·µ¢):

1. Calcular la salida: y·µ¢ = f(w^T x·µ¢ + b)
2. Calcular el error: e·µ¢ = t·µ¢ - y·µ¢
3. Actualizar pesos: w = w + Œ∑ √ó e·µ¢ √ó x·µ¢
4. Actualizar sesgo: b = b + Œ∑ √ó e·µ¢

Donde:
- Œ∑ (eta) es la tasa de aprendizaje
- t·µ¢ es la salida deseada
- y·µ¢ es la salida obtenida

### Pseudoc√≥digo del Algoritmo

```
1. Inicializar pesos w y sesgo b aleatoriamente
2. Establecer tasa de aprendizaje Œ∑
3. Repetir hasta convergencia:
   a. Para cada ejemplo (x·µ¢, t·µ¢):
      - Calcular y·µ¢ = f(w^T x·µ¢ + b)
      - Si y·µ¢ ‚â† t·µ¢:
        * w = w + Œ∑(t·µ¢ - y·µ¢)x·µ¢
        * b = b + Œ∑(t·µ¢ - y·µ¢)
4. Retornar w y b finales
```

## Limitaciones del Perceptr√≥n Simple

### Teorema de Convergencia
- El perceptr√≥n converge en un n√∫mero finito de pasos si los datos son **linealmente separables**
- Si los datos no son linealmente separables, el algoritmo no converge

### Problema XOR
El perceptr√≥n simple no puede resolver problemas no linealmente separables como la funci√≥n XOR:

| x‚ÇÅ | x‚ÇÇ | XOR |
|----|----|----|
| 0  | 0  | 0  |
| 0  | 1  | 1  |
| 1  | 0  | 1  |
| 1  | 1  | 0  |

## Ejemplo Pr√°ctico: Compuerta AND

Vamos a entrenar un perceptr√≥n para implementar la funci√≥n l√≥gica AND.

### Datos de Entrenamiento
| x‚ÇÅ | x‚ÇÇ | AND |
|----|----|----|
| 0  | 0  | 0  |
| 0  | 1  | 0  |
| 1  | 0  | 0  |
| 1  | 1  | 1  |

### Implementaci√≥n Paso a Paso

**Inicializaci√≥n:**
- w‚ÇÅ = 0.5, w‚ÇÇ = 0.5, b = -0.7
- Œ∑ = 0.1

**Iteraci√≥n 1:**
1. (0,0) ‚Üí z = 0.5√ó0 + 0.5√ó0 - 0.7 = -0.7 ‚Üí y = 0 ‚úì
2. (0,1) ‚Üí z = 0.5√ó0 + 0.5√ó1 - 0.7 = -0.2 ‚Üí y = 0 ‚úì
3. (1,0) ‚Üí z = 0.5√ó1 + 0.5√ó0 - 0.7 = -0.2 ‚Üí y = 0 ‚úì
4. (1,1) ‚Üí z = 0.5√ó1 + 0.5√ó1 - 0.7 = 0.3 ‚Üí y = 1 ‚úì

El perceptr√≥n ya clasifica correctamente la funci√≥n AND.

## Ejercicios Propuestos

### Ejercicio 1: Compuerta OR
Entrena un perceptr√≥n para implementar la funci√≥n l√≥gica OR.

**Datos:**
| x‚ÇÅ | x‚ÇÇ | OR |
|----|----|----|
| 0  | 0  | 0 |
| 0  | 1  | 1 |
| 1  | 0  | 1 |
| 1  | 1  | 1 |

### Ejercicio 2: Clasificaci√≥n de Puntos
Dado el conjunto de puntos:
- Clase A: (1,1), (2,2), (3,3)
- Clase B: (-1,-1), (-2,-2), (-3,-3)

Entrena un perceptr√≥n para separar estas dos clases.

### Ejercicio 3: An√°lisis de Convergencia
¬øPor qu√© el perceptr√≥n simple no puede resolver el problema XOR? Demuestra gr√°ficamente que no existe una l√≠nea recta que separe las clases.

## Aplicaciones Hist√≥ricas

1. **Reconocimiento de caracteres**: Uno de los primeros usos del perceptr√≥n
2. **Clasificaci√≥n de patrones simples**: Separaci√≥n de objetos en im√°genes
3. **Filtros de spam**: Clasificaci√≥n b√°sica de correos electr√≥nicos

## Conexi√≥n con Temas Futuros

El perceptr√≥n simple es la base para entender:
- **Perceptr√≥n multicapa** (Semana 3-4)
- **Algoritmo de retropropagaci√≥n**
- **Redes neuronales profundas**
- **Funciones de activaci√≥n m√°s complejas**

## Lecturas Recomendadas

1. Rosenblatt, F. (1958). "The perceptron: a probabilistic model for information storage and organization in the brain"
2. Minsky, M. & Papert, S. (1969). "Perceptrons" - Cap√≠tulo sobre limitaciones
3. Haykin, S. "Neural Networks and Learning Machines" - Cap√≠tulo 1

## Resumen Ejecutivo para Estudio

### Puntos Clave que Debes Recordar üìã

1. **Ecuaci√≥n fundamental**: y = f(w^T x + b)
2. **Regla de aprendizaje**: w = w + Œ∑(t - y)x
3. **Condici√≥n de convergencia**: datos linealmente separables
4. **Limitaci√≥n principal**: no resuelve XOR (problema no lineal)

### Conceptos Cr√≠ticos para Examen ‚≠ê

| Concepto | Definici√≥n | Importancia |
|----------|------------|-------------|
| Hiperplano | Frontera de decisi√≥n en n-1 dimensiones | Separar clases |
| √âpoca | Una pasada completa por todos los datos | Medir progreso |
| Excitaci√≥n (h) | Suma ponderada de entradas | Input de activaci√≥n |
| Linealmente separable | Clases separables por l√≠nea recta | Condici√≥n convergencia |

### F√≥rmulas Esenciales üìê

```
Entrada neta:     z = Œ£(w·µ¢x·µ¢) + b
Funci√≥n escal√≥n:  f(z) = 1 si z‚â•0, 0 si z<0
Actualizaci√≥n:    w·µ¢(nuevo) = w·µ¢(viejo) + Œ∑(target - output)x·µ¢
Error:           e = target - output
```

### Algoritmo del Perceptr√≥n (Resumen) üîÑ

1. **Inicializar** pesos aleatoriamente
2. **Para cada ejemplo**:
   - Calcular salida
   - Si hay error ‚Üí actualizar pesos
3. **Repetir** hasta convergencia o m√°ximo √©pocas

### Casos de Uso vs Limitaciones ‚úÖ‚ùå

**‚úÖ Puede resolver:**
- AND, OR, NOT
- Clasificaci√≥n binaria lineal
- Reconocimiento de patrones simples

**‚ùå NO puede resolver:**
- XOR (requiere perceptr√≥n multicapa)
- Problemas no lineales
- Clasificaci√≥n multi-clase directa

## Preguntas de Repaso

1. ¬øCu√°l es la diferencia entre un perceptr√≥n y una neurona biol√≥gica?
2. ¬øQu√© significa que un conjunto de datos sea "linealmente separable"?
3. ¬øPor qu√© es importante la tasa de aprendizaje Œ∑?
4. ¬øC√≥mo afecta la inicializaci√≥n de pesos al proceso de aprendizaje?
5. ¬øCu√°ndo est√° garantizada la convergencia del algoritmo del perceptr√≥n?

## Preguntas Adicionales de Estudio

6. ¬øPor qu√© el perceptr√≥n simple no puede resolver XOR?
7. ¬øQu√© papel juega el sesgo (bias) en la capacidad de clasificaci√≥n?
8. ¬øC√≥mo se relaciona el hiperplano con la frontera de decisi√≥n?
9. ¬øQu√© sucede si la tasa de aprendizaje es muy alta o muy baja?
10. ¬øCu√°l es la diferencia entre √©poca e iteraci√≥n?

## Trucos de Estudio y Mnemotecnias üß†

### Recordar la Regla de Actualizaci√≥n
**"WETŒ∑"**: **W**eights = **E**rror √ó **T**arget √ó **Œ∑**
- Si error > 0: incrementar peso
- Si error < 0: decrementar peso
- Si error = 0: no cambiar peso

### Visualizaci√≥n del Hiperplano
- **2D**: Imagina una l√≠nea que separa c√≠rculos rojos de azules
- **3D**: Imagina un papel que separa pelotas rojas de azules
- **nD**: Generalizaci√≥n matem√°tica del concepto anterior

### Nemotecnia para Convergencia
**"LINEAL"**: **L**os datos **I**nealmente separables **N**ecesitan **E**l **A**lgoritmo para **L**ograra convergencia

### Regla de Oro del Perceptr√≥n
> "Si no lo puedes separar con una l√≠nea recta, el perceptr√≥n simple no lo puede aprender"

## Ejercicios R√°pidos de Repaso ‚ö°

### Test de 5 Minutos
1. Dibuja un perceptr√≥n con 2 entradas
2. Escribe la ecuaci√≥n de la entrada neta
3. ¬øQu√© pasa si w‚ÇÅ=0.5, w‚ÇÇ=0.3, b=-0.2 y x=[1,1]?
4. ¬øEl punto (1,1) pertenece a la clase 1 o 0?

### Respuestas R√°pidas
1. [Entrada1]‚Üíw‚ÇÅ‚Üí[+]‚Üíf()‚Üí[Salida]
   [Entrada2]‚Üíw‚ÇÇ‚Üí‚Üó  ‚Üë
   [Bias]‚Üíb‚Üí-----‚Üí 
2. z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + b
3. z = 0.5(1) + 0.3(1) + (-0.2) = 0.6
4. Clase 1 (porque z ‚â• 0)
