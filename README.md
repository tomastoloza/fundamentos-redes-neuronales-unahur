# Fundamentos de Redes Neuronales - UNAHUR

## DescripciÃ³n

Repositorio reorganizado siguiendo principios **Clean Code** y **SOLID** para los trabajos prÃ¡cticos de la materia Fundamentos de Redes Neuronales de la Universidad Nacional de Hurlingham (UNAHUR).

## ğŸ—ï¸ Nueva Estructura Organizada

```
fundamentos-redes-neuronales/
â”œâ”€â”€ comun/                           # MÃ³dulos compartidos
â”‚   â”œâ”€â”€ constantes/                  # Constantes centralizadas
â”‚   â”‚   â”œâ”€â”€ constantes_redes_neuronales.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ src/                         # CÃ³digo fuente compartido
â”‚   â”‚   â”œâ”€â”€ funciones_activacion.py  # Funciones de activaciÃ³n (SRP)
â”‚   â”‚   â”œâ”€â”€ utilidades_matematicas.py # Utilidades matemÃ¡ticas (SRP)
â”‚   â”‚   â”œâ”€â”€ evaluador_rendimiento.py # EvaluaciÃ³n de rendimiento (SRP)
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ tp1/                             # PerceptrÃ³n Simple
â”‚   â”œâ”€â”€ src/                         # CÃ³digo fuente TP1
â”‚   â”‚   â”œâ”€â”€ perceptron_simple.py     # ImplementaciÃ³n perceptrÃ³n simple
â”‚   â”‚   â”œâ”€â”€ cargador_datos.py        # Carga de datos (SRP)
â”‚   â”‚   â”œâ”€â”€ entrenador_compuertas.py # Entrenamiento compuertas (SRP)
â”‚   â”‚   â”œâ”€â”€ visualizador_resultados.py # VisualizaciÃ³n (SRP)
â”‚   â”‚   â”œâ”€â”€ main_tp1.py             # Ejecutor principal
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ datos/                       # Datos de TP1
â”‚   â”œâ”€â”€ resultados/                  # Resultados de experimentos
â”‚   â”œâ”€â”€ tests/                       # Tests unitarios
â”‚   â””â”€â”€ README.md                    # DocumentaciÃ³n TP1
â”œâ”€â”€ tp2/                             # PerceptrÃ³n Multicapa
â”‚   â”œâ”€â”€ src/                         # CÃ³digo fuente TP2
â”‚   â”‚   â”œâ”€â”€ perceptron_multicapa.py  # ImplementaciÃ³n multicapa
â”‚   â”‚   â”œâ”€â”€ cargador_datos_digitos.py # Carga datos dÃ­gitos (SRP)
â”‚   â”‚   â”œâ”€â”€ entrenador_tp2.py        # Entrenamiento TP2 (SRP)
â”‚   â”‚   â”œâ”€â”€ main_tp2.py             # Ejecutor principal
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ datos/                       # Datos de TP2
â”‚   â”œâ”€â”€ resultados/                  # Resultados de experimentos
â”‚   â”œâ”€â”€ tests/                       # Tests unitarios
â”‚   â””â”€â”€ README.md                    # DocumentaciÃ³n TP2
â””â”€â”€ README.md                        # Esta documentaciÃ³n
```

## ğŸ¯ Principios Aplicados

### Clean Code
- âœ… **EliminaciÃ³n de nÃºmeros mÃ¡gicos**: Todas las constantes centralizadas
- âœ… **Nombres descriptivos**: Variables y mÃ©todos con nombres claros en espaÃ±ol
- âœ… **Funciones pequeÃ±as**: Cada funciÃ³n tiene una responsabilidad especÃ­fica
- âœ… **SeparaciÃ³n de concerns**: LÃ³gica separada por responsabilidades
- âœ… **CÃ³digo autodocumentado**: Nombres explicativos y documentaciÃ³n clara

### Principios SOLID

#### Single Responsibility Principle (SRP)
- `PerceptronSimple` / `PerceptronMulticapa`: Solo lÃ³gica de redes neuronales
- `CargadorDatos` / `CargadorDatosDigitos`: Solo carga y preparaciÃ³n de datos
- `EntrenadorCompuertas` / `EntrenadorTP2`: Solo lÃ³gica de entrenamiento
- `VisualizadorResultados`: Solo presentaciÃ³n y visualizaciÃ³n
- `EvaluadorRendimiento`: Solo mÃ©tricas y evaluaciÃ³n
- `FuncionesActivacion`: Solo funciones de activaciÃ³n
- `UtilidadesMatematicas`: Solo operaciones matemÃ¡ticas

#### Open/Closed Principle (OCP)
- Extensible para nuevas funciones de activaciÃ³n sin modificar cÃ³digo existente
- Nuevas arquitecturas de red pueden agregarse sin cambiar implementaciones base
- Nuevos tipos de problemas pueden aÃ±adirse extendiendo los entrenadores

#### Dependency Inversion Principle (DIP)
- InyecciÃ³n de dependencias entre componentes principales
- Abstracciones no dependen de detalles concretos
- FÃ¡cil intercambio de implementaciones para testing

## ğŸš€ Uso RÃ¡pido

### TP1 - PerceptrÃ³n Simple
```python
from tp1.src.main_tp1 import EjecutorTP1

# Ejecutar todos los experimentos del TP1
ejecutor = EjecutorTP1()
ejecutor.ejecutar_todos_los_experimentos()
```

### TP2 - PerceptrÃ³n Multicapa
```python
from tp2.src.main_tp2 import EjecutorTP2

# Ejecutar todos los experimentos del TP2
ejecutor = EjecutorTP2()
ejecutor.ejecutar_todos_los_experimentos()
```

## ğŸ“š Experimentos Incluidos

### TP1 - PerceptrÃ³n Simple
- **Compuertas LÃ³gicas**: AND, OR, XOR (anÃ¡lisis de separabilidad lineal)
- **Funciones de ActivaciÃ³n**: EscalÃ³n, sigmoide, lineal
- **Datos desde Archivo**: AnÃ¡lisis de generalizaciÃ³n
- **ComparaciÃ³n**: PerceptrÃ³n lineal vs no lineal

### TP2 - PerceptrÃ³n Multicapa
- **Problema XOR**: DemostraciÃ³n de capacidad no lineal
- **DiscriminaciÃ³n Pares**: ClasificaciÃ³n binaria de dÃ­gitos
- **ClasificaciÃ³n 10 Clases**: Reconocimiento multiclase de dÃ­gitos
- **EvaluaciÃ³n con Ruido**: AnÃ¡lisis de robustez
- **ComparaciÃ³n Arquitecturas**: Diferentes configuraciones de red

## ğŸ”§ Componentes Compartidos

### Constantes Centralizadas
```python
from comun.constantes.constantes_redes_neuronales import (
    TASA_APRENDIZAJE_DEFECTO,
    EPOCAS_MAXIMAS_DEFECTO,
    ARQUITECTURAS_TP2
)
```

### Utilidades MatemÃ¡ticas
```python
from comun.src.utilidades_matematicas import UtilidadesMatematicas

# InicializaciÃ³n de pesos
pesos = UtilidadesMatematicas.inicializar_pesos_xavier(filas, columnas)

# DivisiÃ³n de datos
train_x, train_y, test_x, test_y = UtilidadesMatematicas.dividir_datos_entrenamiento_prueba(X, y)
```

### Funciones de ActivaciÃ³n
```python
from comun.src.funciones_activacion import FuncionesActivacion

# Obtener funciÃ³n y su derivada
funcion, derivada = FuncionesActivacion.obtener_funcion_y_derivada('sigmoide')
```

## ğŸ“Š Resultados y AnÃ¡lisis

### Conclusiones TP1
- **Separabilidad Lineal**: PerceptrÃ³n simple solo resuelve problemas linealmente separables
- **XOR**: Requiere arquitecturas mÃ¡s complejas (multicapa)
- **GeneralizaciÃ³n**: ValidaciÃ³n independiente es crucial

### Conclusiones TP2
- **Superioridad Multicapa**: Resuelve problemas no linealmente separables
- **Sobreajuste**: Arquitecturas complejas con datos limitados memorizan sin generalizar
- **Robustez**: Redes bien entrenadas son resistentes a ruido pequeÃ±o

## ğŸ› ï¸ InstalaciÃ³n y Dependencias

```bash
# Clonar repositorio
git clone <repository-url>
cd fundamentos-redes-neuronales

# Instalar dependencias
pip install numpy

# Ejecutar experimentos
python tp1/src/main_tp1.py
python tp2/src/main_tp2.py
```

## ğŸ“ˆ Mejoras Implementadas

### Antes de la ReorganizaciÃ³n
- âŒ CÃ³digo monolÃ­tico en archivos grandes
- âŒ NÃºmeros mÃ¡gicos dispersos por el cÃ³digo
- âŒ Responsabilidades mezcladas en una sola clase
- âŒ DifÃ­cil mantenimiento y extensiÃ³n
- âŒ DuplicaciÃ³n de cÃ³digo entre proyectos

### DespuÃ©s de la ReorganizaciÃ³n
- âœ… **SeparaciÃ³n clara de responsabilidades** (SRP)
- âœ… **Constantes centralizadas** (eliminaciÃ³n de nÃºmeros mÃ¡gicos)
- âœ… **CÃ³digo reutilizable** entre TP1 y TP2
- âœ… **FÃ¡cil extensiÃ³n** para nuevas funcionalidades (OCP)
- âœ… **Mantenimiento simplificado** (cada clase tiene un propÃ³sito claro)
- âœ… **Testing independiente** de componentes
- âœ… **DocumentaciÃ³n completa** en espaÃ±ol

## ğŸ§ª Testing

```python
# Ejemplo de test unitario para componente individual
from tp1.src.perceptron_simple import PerceptronSimple

def test_perceptron_and():
    perceptron = PerceptronSimple(num_entradas=2, funcion_activacion='escalon')
    # Test especÃ­fico para compuerta AND
    assert perceptron is not None
```

## ğŸ¤ ContribuciÃ³n

Para mantener la calidad del cÃ³digo reorganizado:

1. **Seguir principios SOLID**: Cada clase debe tener una responsabilidad Ãºnica
2. **Usar constantes centralizadas**: No agregar nÃºmeros mÃ¡gicos al cÃ³digo
3. **Documentar en espaÃ±ol**: Mantener consistencia en el idioma
4. **Separar responsabilidades**: No mezclar lÃ³gica de datos, entrenamiento y visualizaciÃ³n
5. **Agregar tests**: Para nuevas funcionalidades

## ğŸ“ Licencia

Proyecto acadÃ©mico - Universidad Nacional de Hurlingham (UNAHUR)
Materia: Fundamentos de Redes Neuronales

## ğŸ‘¥ Autores

- **Estudiantes**: SebastiÃ¡n Brandariz, Mauricio Challiol, TomÃ¡s Toloza
- **Docente**: Emiliano Churruca
- **ReorganizaciÃ³n**: AplicaciÃ³n de principios Clean Code y SOLID

---

## ğŸ“ Valor AcadÃ©mico

Esta reorganizaciÃ³n demuestra:

- **AplicaciÃ³n prÃ¡ctica** de principios de ingenierÃ­a de software
- **CÃ³digo mantenible** y extensible para proyectos acadÃ©micos
- **SeparaciÃ³n de concerns** en proyectos de machine learning
- **ReutilizaciÃ³n de cÃ³digo** entre diferentes experimentos
- **DocumentaciÃ³n profesional** de proyectos tÃ©cnicos

El cÃ³digo resultante es mÃ¡s **legible**, **mantenible** y **extensible**, siguiendo las mejores prÃ¡cticas de la industria del software.
