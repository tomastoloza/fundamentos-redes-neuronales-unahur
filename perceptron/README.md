# Perceptr√≥n Simple - Implementaci√≥n Modular

Este proyecto implementa el algoritmo del perceptr√≥n siguiendo el pseudoc√≥digo proporcionado, aplicado a diferentes problemas de aprendizaje autom√°tico.

## üìÅ Estructura de Archivos

### Archivos Principales

- **`main.py`** - Programa principal con men√∫ interactivo
- **`perceptron_unificado.py`** - Implementaci√≥n unificada del algoritmo del perceptr√≥n
- **`compuerta_and.py`** - Implementaci√≥n de la compuerta l√≥gica AND
- **`compuerta_or.py`** - Implementaci√≥n de la compuerta l√≥gica OR  
- **`tp1.py`** - Implementaci√≥n para regresi√≥n (TP1-EJ2)

### Archivos de Datos

- **`TP1-ej2-Conjunto-entrenamiento.txt`** - Datos de entrenamiento para TP1
- **`TP1-ej2-Salida-deseada.txt`** - Salidas esperadas para TP1

## üöÄ Uso

### Ejecutar el Programa Principal

```bash
python3 main.py
```

### Ejecutar M√≥dulos Individuales

```bash
# Compuerta AND (lineal)
python3 compuerta_and.py

# Compuerta AND (no lineal)
python3 compuerta_and.py no_lineal

# Compuerta OR (lineal)
python3 compuerta_or.py

# Compuerta OR (no lineal)
python3 compuerta_or.py no_lineal

# Comparar ambos tipos
python3 compuerta_and.py comparar

# TP1 (Regresi√≥n)
python3 tp1.py
```

## üîß Algoritmo Implementado

El perceptr√≥n unificado utiliza el siguiente algoritmo base con selecci√≥n aleatoria de ejemplos:

```python
while True:
    # Calcular error global para todo el conjunto
    error_global = calcular_error(entradas, salidas_deseadas, funcion_activacion)
    
    # Criterio de parada
    if error_global < error_min or epoca >= max_epocas:
        break
    
    # Selecci√≥n aleatoria de un ejemplo
    indice = random.randint(0, len(entradas))
    entrada = append(entradas[indice], 1)  # Agregar sesgo
    
    # Calcular salida
    h = dot(entrada, pesos)
    O = funcion_activacion(h)
    
    # Calcular error
    M = salidas_deseadas[indice] - O
    
    # Actualizar pesos (gradiente descendente)
    delta_W = tasa_aprendizaje * M * derivada_activacion(h) * entrada
    pesos += delta_W
```

### Funciones de Activaci√≥n Disponibles

- **`escalon`**: f(x) = 1 if x ‚â• 0 else 0 (perceptr√≥n lineal cl√°sico)
- **`sigmoide`**: f(x) = 1/(1 + e^(-2x)) (perceptr√≥n no lineal)
- **`lineal`**: f(x) = x (regresi√≥n)

## üìã TRABAJO PR√ÅCTICO - ENUNCIADO COMPLETO

### 1. Perceptr√≥n Simple con Funci√≥n de Activaci√≥n Escal√≥n

**Implementar el algoritmo de perceptr√≥n simple con funci√≥n de activaci√≥n escal√≥n y utilizarlo para aprender los siguientes problemas:**

#### Problema 1: Funci√≥n L√≥gica 'Y' (AND)
- **Entradas**: x = {{-1, 1}, {1, -1}, {-1, -1}, {1, 1}}
- **Salida esperada**: y = {-1, -1, -1, 1}

#### Problema 2: Funci√≥n L√≥gica 'O exclusivo' (XOR)
- **Entradas**: x = {{-1, 1}, {1, -1}, {-1, -1}, {1, 1}}
- **Salida esperada**: y = {1, 1, -1, -1}

**Pregunta de An√°lisis:**
¬øQu√© puede decir acerca de los problemas que puede resolver el perceptr√≥n simple escal√≥n en relaci√≥n a la resoluci√≥n de los problemas que se le pidi√≥ que haga que el perceptr√≥n aprenda?

### 2. Perceptr√≥n Simple Lineal y No Lineal

**Implementar el algoritmo del perceptr√≥n simple lineal y perceptr√≥n simple no lineal y utilizarlos para aprender el problema especificado en los archivos:**
- `TP1-ej2-Conjunto-entrenamiento.txt`
- `TP1-ej2-Salida-deseada.txt`

#### Evaluaciones Requeridas:

- **Evaluar la capacidad** del perceptr√≥n simple lineal y perceptr√≥n simple no lineal para aprender la funci√≥n cuyas muestras est√°n presentes en los archivos indicados.

- **Evaluar la capacidad de generalizaci√≥n** del perceptr√≥n simple no lineal utilizando, de los datos provistos, un subconjunto de ellos para entrenar y otro subconjunto para testear.

#### Preguntas de An√°lisis:

- **¬øC√≥mo podr√≠a escoger el mejor conjunto de entrenamiento?**
- **¬øC√≥mo podr√≠a evaluar la m√°xima capacidad de generalizaci√≥n del perceptr√≥n para este conjunto de datos?**

### 3. Presentaci√≥n del Trabajo

**Fecha de Presentaci√≥n:** 8 de abril

**Formato:** PowerPoint o programa similar

**Contenido Requerido:**
- T√≠tulo del trabajo
- Nombre de la materia
- Nombre de los integrantes del grupo
- Fecha

**Estructura de la Presentaci√≥n:**

Para cada √≠tem solicitado:
1. **Comentar lo que se hizo** y las decisiones tomadas para llevarlo a cabo
2. **Exponer las dificultades** que se presentaron (si correspondiera)
3. **Exponer los resultados**
4. **Conclusiones del trabajo** al finalizar la presentaci√≥n

## üéØ Implementaci√≥n Realizada

### 1. Funciones L√≥gicas con Perceptr√≥n Escal√≥n

#### Funci√≥n AND (Y)
- ‚úÖ **Implementada** en `compuerta_and.py`
- ‚úÖ **Convergencia**: T√≠pica en 10-50 √©pocas
- ‚úÖ **Resultado**: 100% de precisi√≥n (problema linealmente separable)

#### Funci√≥n XOR (O exclusivo)
- ‚ö†Ô∏è **Limitaci√≥n conocida**: El perceptr√≥n simple NO puede resolver XOR
- üìù **Raz√≥n**: XOR no es linealmente separable
- üîç **An√°lisis**: Requiere perceptr√≥n multicapa o funciones no lineales

### 2. Perceptr√≥n para Regresi√≥n (TP1-EJ2)

#### Perceptr√≥n Lineal (Regresi√≥n)
- ‚úÖ **Implementado** con funci√≥n de activaci√≥n lineal
- ‚úÖ **Datos**: 200 ejemplos, 3 caracter√≠sticas
- ‚úÖ **Divisi√≥n**: 80% entrenamiento, 20% prueba
- ‚úÖ **Normalizaci√≥n**: Z-score para mejorar convergencia

#### Perceptr√≥n No Lineal (Sigmoide)
- ‚úÖ **Implementado** con funci√≥n sigmoide
- ‚úÖ **Gradiente descendente** con derivada de sigmoide
- ‚úÖ **Convergencia** por umbral de error MSE

### 3. Capacidad de Generalizaci√≥n

#### Estrategias Implementadas:
- **Divisi√≥n aleatoria** de datos (80/20)
- **Validaci√≥n cruzada** posible con m√∫ltiples ejecuciones
- **Normalizaci√≥n** de datos para estabilidad
- **Early stopping** para evitar sobreajuste

#### M√©tricas de Evaluaci√≥n:
- **Error cuadr√°tico medio (MSE)**
- **Convergencia en √©pocas**
- **Comparaci√≥n entre tipos de perceptr√≥n**

## üìä Resultados T√≠picos

### Compuertas L√≥gicas
- **AND Lineal**: Convergencia en 10-50 √©pocas, error = 0.000000
- **AND No Lineal**: Convergencia en 300-600 √©pocas, error < 0.01
- **OR Lineal**: Convergencia en 3-10 √©pocas, error = 0.000000
- **XOR**: No converge (limitaci√≥n te√≥rica del perceptr√≥n simple)

### TP1-EJ2 (Regresi√≥n)
- **Perceptr√≥n Lineal**: MSE final ~50-100 (dependiente de datos)
- **Perceptr√≥n No Lineal**: Mejor capacidad de ajuste a patrones complejos
- **Generalizaci√≥n**: Evaluada mediante conjunto de prueba separado

## üîç An√°lisis de Limitaciones

### Perceptr√≥n Simple Escal√≥n
- ‚úÖ **Puede resolver**: Problemas linealmente separables (AND, OR)
- ‚ùå **No puede resolver**: Problemas no linealmente separables (XOR)
- üìù **Implicaci√≥n**: Limitado a funciones booleanas linealmente separables

### Selecci√≥n del Mejor Conjunto de Entrenamiento
- **Diversidad**: Representar toda la distribuci√≥n de datos
- **Tama√±o**: Balance entre informaci√≥n suficiente y eficiencia
- **Aleatoriedad**: Evitar sesgos en la selecci√≥n
- **Estratificaci√≥n**: Mantener proporciones de clases/rangos

### Evaluaci√≥n de M√°xima Capacidad de Generalizaci√≥n
- **Validaci√≥n cruzada k-fold**
- **Curvas de aprendizaje** (error vs. tama√±o del conjunto)
- **An√°lisis de sesgo-varianza**
- **Pruebas con m√∫ltiples divisiones aleatorias**

## üõ†Ô∏è Dependencias

```bash
pip install numpy
```

## üéì Conclusiones del Trabajo

1. **Limitaciones del Perceptr√≥n Simple**: No puede resolver problemas no linealmente separables como XOR
2. **Ventajas del Perceptr√≥n No Lineal**: Mayor capacidad de modelado para patrones complejos
3. **Importancia de la Preparaci√≥n de Datos**: Normalizaci√≥n y divisi√≥n adecuada mejoran significativamente el rendimiento
4. **Trade-off Sesgo-Varianza**: El perceptr√≥n lineal tiene mayor sesgo pero menor varianza que el no lineal
5. **Aplicabilidad**: El perceptr√≥n simple es efectivo para problemas de clasificaci√≥n binaria linealmente separables y regresi√≥n lineal b√°sica